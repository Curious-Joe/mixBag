% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PM_util_functions.R
\name{compare_classifiers}
\alias{compare_classifiers}
\title{Comparing Classifiers' Performance}
\usage{
compare_classifiers(recipe, test_df, target_lab = Y)
}
\arguments{
\item{recipe}{A parsnip recipe object.}

\item{test_df}{Test data frame to test model performances.}

\item{target_lab}{Positive label used in the target feature.}
}
\value{
Returns following two things:
\itemize{
\item A line plot showing comparative area under the ROC curve score
\item A tiblle containing the test data along with the predicted probability calculated by the five classifiers.
}
}
\description{
Fits five commonly used classifiers on data and compare their performance based on AROC.
}
\details{
\itemize{
\item Make sure the recipe object entered is at pre-prepped stage
\item Five models compared are: logistic regression, elastic net, random forest, support vector machine, xtreme gradient boosting.
}
}
\examples{
  library(tidymodels)
  split <- rsample::initial_split(wine, strata = quality_bin)
  train <- rsample::training(split)
  test <- rsample::testing(split)
  recipe <- recipes::recipe(quality_bin ~ ., data = train) \%>\%
  step_string2factor(all_nominal()) \%>\%
  step_knnimpute(all_predictors()) \%>\%
  step_normalize(all_numeric())

  compare_classifiers(recipe = recipe, test_df = test, target_lab = 1)

}
